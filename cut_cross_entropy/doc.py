# Copyright (C) 2024 Apple Inc. All Rights Reserved.
from cut_cross_entropy.cce_utils import LinearCrossEntropyImpl

LINEAR_CROSS_ENTROPY_DOC = """Computes cross-entropy loss using the logits generated by performing
    the matrix multiplication between the embeddings (e) and classifier (c).

    This method saves GPU memory by not materializing the logits into GPU
    main memory.


    Specifically, this computes

    ```python

    loss = F.cross_entropy((e @ c.T).float(), targets)
    ```

    without allocating the intermediary (e @ c.T).float() matrix.

    :param e: Embedding of the inputs used to compute the logits. Shape (..., D)
    :param c: Classifier matrix. Shape (NumClasses, D). Can be a DTensor. See the note for semantics.
    :param targets: The target class for each input. Values must be in [0, NumClasses). Shape (...)
    :param bias: An optional bias. Shape (NumClasses). Can be a DTensor.
    :param ignore_index: If an input as a target of this value, it is ignored in the loss computation.
    :param softcap: The value for logit softcapping.
    :param reduction: The reduction to perform over the loss. Supports "mean", "sum", and "none".
    :param shift: When non-zero, the embedding and targets will be shifted along the temporal axis to perform nth-next token prediction.
        Specifically, this is used to efficiently compute the following

        ```python
        shift_e = e[..., :-shift, :].flatten(0, -2)
        shift_targets = targets[..., shift:].flatten()

        loss = F.cross_entropy((shift_e @ c.T).float(), targets)
        ```

        If given a boolean value, False will be treated as zero and True will be treated as one.

        When this value is non-zero or True, e and targets must have shape (..., T, D) and (..., T), respectively.

        Integer values must be in [0, T)
    :param return_lse: Whether or not to return the log-sum-exp. Useful for computing Z loss.
"""

CCE_OPTS_DOC = [
    """
    :param filter_eps: The threshold value used to determine which locations can be safely ignored
        in gradient computation. The default value of "auto" will automatically choose a value
        based on the input dtype.""",
    """
    :param accum_e_fp32: Whether or not to use fp32 accumulation for dE (use Kahan summation for Triton < 3.2 to work around a bug).
        This is useful when working with models with a very large vocabulary or very long sequence lengths.""",
    """
    :param accum_c_fp32: Whether or not to use fp32 accumulation for dC (use Kahan summation for Triton < 3.2 to work around a bug).
        This is useful when working with models with a very large vocabulary or very long sequence lengths.""",
    """
    :param filter_e_grad: Whether or not to apply gradient filter to the embedding gradient (dE). If filter_eps is None, this
        will be set to False.""",
    """
    :param filter_c_grad: Whether or not to apply gradient filter to the classifier gradient (dC). If filter_eps is None, this
        will be set to False.""",
]

IMPL_DOC = """
    :param impl: The implementation to use. Can be one of
        {impls}.
        If using one of the 'cce' implementations that is not 'cce',
        the cce parameters will be set automatically and any use
        provided values be ignored.
""".format(impls=set(v.name.lower() for v in LinearCrossEntropyImpl))

DTENSOR_NOTE = """
Note:
    When c and the optional bias are given as a torch.distributed.tensor.DTensor,
    the assumption is that these are tensors produced by PyTorch's fully_shard and thus
    data-parallel semantics apply.

    CCE will do the following:
        1. Gather the full tensor.
        2. Cast to the same dtype as e.

    On backward the following will be done:
        1. The gradient will be casted back to original dtype
        2. The gradient will be all-reduced
        3. The all-reduced gradient will be sharded to create the DTensor's gradient.
"""


def add_doc_start(*docstr: str):
    def add_doc(fn):
        fn.__doc__ = "".join(docstr) + (fn.__doc__ if fn.__doc__ is not None else "")

        return fn

    return add_doc


def add_doc_end(*docstr: str):
    def add_doc(fn):
        fn.__doc__ = (fn.__doc__ if fn.__doc__ is not None else "") + "".join(docstr)

        return fn

    return add_doc
